{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrrong2020/A/blob/main/%E2%80%9CFYP_Pro_T4_overfit_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjeGEXN7MH4C"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "!sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "!sudo apt update >/dev/null 2>&1\n",
        "!sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "!google-drive-ocamlfuse\n",
        "!sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser\n",
        "!xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser\n",
        "%cd /content\n",
        "!mkdir gdrive\n",
        "%cd gdrive\n",
        "!mkdir \"MyDrive\"\n",
        "!google-drive-ocamlfuse \"/content/gdrive/MyDrive\""
      ],
      "metadata": {
        "id": "t7HXD6VAQV-0",
        "outputId": "c1199c7e-8a1d-4aa3-b397-1da8d14f9996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‘gdrive’: File exists\n",
            "/content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT_n6JPqMeCj"
      },
      "source": [
        "Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWJglFbYMmEr"
      },
      "outputs": [],
      "source": [
        "!cp /content/gdrive/MyDrive/Kaggle/unzip/data.zip /content\n",
        "\n",
        "!unzip -nq /content/data.zip\n",
        "\n",
        "!rm /content/data.zip\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Y1lyGWiJAU"
      },
      "source": [
        "#GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqAFoYKfjQHr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #For reading csv files.\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt #For plotting.\n",
        "import PIL.Image as Image #For working with image files.\n",
        "\n",
        "#Importing torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader #For working with data.\n",
        "from torchvision import models,transforms #For pretrained models,image transformations.\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "#manual seed for reproductivity and potential performance improvement\n",
        "torch.manual_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-dPSwwujHvj"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Use GPU if it's available or else use CPU.\n",
        "print(device) #Prints the device we're using."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/gdrive/MyDrive/Kaggle/unzip/allLabels.csv /content"
      ],
      "metadata": {
        "id": "Vgy874jFDdQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjkojF_kawHd"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = \"/content/\"\n",
        "\n",
        "#label csv\n",
        "all_df = pd.read_csv(f\"{path}allLabels.csv\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# subset for hyperparameters tuning\n",
        "\n",
        "part = 0.80\n",
        "train_df, drop_df = train_test_split(all_df, test_size=part, random_state=42)\n",
        "test_sample_size = round(7000 * (1 - part) )  #roughtly 1/3\n",
        "train_df, test_df = train_test_split(train_df, test_size=test_sample_size, random_state=42)\n",
        "\n",
        "# Assuming train_df is your original training DataFrame\n",
        "# test_sample_size = 7000 # roughtly 1/3\n",
        "# train_df, test_df = train_test_split(all_df, test_size=test_sample_size, random_state=42)\n",
        "\n",
        "# Now split the remaining training data into training and validation sets\n",
        "train_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f'No.of.training_samples: {len(train_df)}')\n",
        "print(f'No.of.testing_samples: {len(test_df)}')\n",
        "print(f'No.of.val_samples: {len(valid_df)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQVZJzjXkBwP"
      },
      "outputs": [],
      "source": [
        "#Histogram of label counts.\n",
        "train_df.level.hist()\n",
        "plt.xticks([0,1,2,3,4])\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYrE034FkXSI"
      },
      "outputs": [],
      "source": [
        "#As you can see,the data is imbalanced.\n",
        "#So we've to calculate weights for each class,which can be used in calculating loss.\n",
        "\n",
        "from sklearn.utils import class_weight #For calculating weights for each class.\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.array([0,1,2,3,4]),y=train_df['level'].values)\n",
        "class_weights = torch.tensor(class_weights,dtype=torch.float).to(device)\n",
        "\n",
        "print(class_weights) #Prints the calculated weights for the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CAOhlpYklys"
      },
      "outputs": [],
      "source": [
        "#For getting a random image from our training set.\n",
        "# num = int(np.random.randint(0,len(train_df)-1,(1,))) #Picks a random number.\n",
        "# sample_image = (f'{path}train/{train_df[\"image\"][num]}.jpeg')#Image file.\n",
        "# sample_image = Image.open(sample_image)\n",
        "# plt.imshow(sample_image)\n",
        "# plt.axis('off')\n",
        "# plt.title(f'Class: {train_df[\"level\"][num]}') #Class of the random image.\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8rgUjyom5uS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class dataset(Dataset): # Inherits from the Dataset class.\n",
        "    '''\n",
        "    dataset class overloads the __init__, __len__, __getitem__ methods of the Dataset class.\n",
        "\n",
        "    Attributes :\n",
        "        df:  DataFrame object for the csv file.\n",
        "        data_path: Location of the dataset.\n",
        "        image_transform: Transformations to apply to the image.\n",
        "        train: A boolean indicating whether it is a training_set or not.\n",
        "    '''\n",
        "\n",
        "    def __init__(self,df,data_path,image_transform=None,train=True): # Constructor.\n",
        "        super(Dataset,self).__init__() #Calls the constructor of the Dataset class.\n",
        "        self.df = df\n",
        "        self.data_path = data_path\n",
        "        self.image_transform = image_transform\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) #Returns the number of samples in the dataset.\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image_id = self.df['image'][index]\n",
        "      image = Image.open(f'{self.data_path}/{image_id}.jpeg')  # Image.\n",
        "\n",
        "      label = self.df['level'][index] if self.train else -1  # Get label if training\n",
        "\n",
        "      # Check if the sample belongs to a minority class and if augmentation should be applied\n",
        "      if self.train and self.df['level'].value_counts()[label] < 1000:\n",
        "          # Apply the augmentations\n",
        "          if random.random() < 0.5:\n",
        "              image = image.rotate(random.uniform(-25, 25))  # Rotate with a random angle between -25 and 25\n",
        "          if random.random() < 0.5:\n",
        "              image = image.transpose(Image.FLIP_LEFT_RIGHT)  # Flip left-right\n",
        "          if random.random() < 0.5:\n",
        "              image = image.transpose(Image.FLIP_TOP_BOTTOM)  # Flip top-bottom\n",
        "\n",
        "      if self.image_transform:\n",
        "          image = self.image_transform(image)  # Applies transformation to the image.\n",
        "\n",
        "      return (image, label) if self.train else image  # Return image and label if training, otherwise image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UsJqLqinRA0"
      },
      "outputs": [],
      "source": [
        "image_transform = transforms.Compose([transforms.Resize([512,512]),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) #Transformations to apply to the image.\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_set = dataset(train_df,f'{path}train',image_transform=image_transform)\n",
        "test_set = dataset(test_df,f'{path}train',image_transform=image_transform)\n",
        "valid_set = dataset(valid_df,f'{path}train',image_transform=image_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm8l4B2Am0Ry"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_set,batch_size=16,shuffle=True, num_workers=4)\n",
        "valid_dataloader = DataLoader(valid_set,batch_size=16,shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_set, batch_size=16, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SXyKyippyDz"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50,ResNet50_Weights\n",
        "\n",
        "model = models.resnet50(weights=ResNet50_Weights.DEFAULT) #Downloads the resnet50 model which is pretrained on Imagenet dataset.\n",
        "# Replace the Final layer of pretrained resnet50\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 5),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r96X5p-_q2YI"
      },
      "outputs": [],
      "source": [
        "model = model.to(device) #Moves the model to the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYwzKdDzqier"
      },
      "outputs": [],
      "source": [
        "def train(dataloader,model,loss_fn,optimizer):\n",
        "    '''\n",
        "    train function updates the weights of the model based on the\n",
        "    loss using the optimizer in order to get a lower loss.\n",
        "\n",
        "    Args :\n",
        "         dataloader: Iterator for the batches in the data_set.\n",
        "         model: Given an input produces an output by multiplying the input with the model weights.\n",
        "         loss_fn: Calculates the discrepancy between the label & the model's predictions.\n",
        "         optimizer: Updates the model weights.\n",
        "\n",
        "    Returns :\n",
        "         Average loss per batch which is calculated by dividing the losses for all the batches\n",
        "         with the number of batches.\n",
        "    '''\n",
        "\n",
        "    model.train() #Sets the model for training.\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    for batch,(x,y) in enumerate(dataloader): #Iterates through the batches.\n",
        "\n",
        "\n",
        "        output = model(x.to(device)) #model's predictions.\n",
        "        loss   = loss_fn(output,y.to(device)) #loss calculation.\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        total        += y.size(0)\n",
        "        predictions   = output.argmax(dim=1).cpu().detach() #Index for the highest score for all the samples in the batch.\n",
        "        correct      += (predictions == y.cpu().detach()).sum().item() #No.of.cases where model's predictions are equal to the label.\n",
        "        accuracy = 100*(correct/total)\n",
        "\n",
        "        optimizer.zero_grad() #Gradient values are set to zero.\n",
        "        loss.backward() #Calculates the gradients.\n",
        "        optimizer.step() #Updates the model weights.\n",
        "\n",
        "        #for memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Print some information every 100 batches\n",
        "        if batch % 70 == 0:\n",
        "            print(f'Batch {batch}/{len(dataloader)} processed, running loss: {running_loss:.6f}, correct predictions: {correct}, total: {total}')\n",
        "\n",
        "\n",
        "    avg_loss = running_loss/len(dataloader) # Average loss for a single batch\n",
        "\n",
        "    print(f'\\nTraining Loss per batch = {avg_loss:.6f}',end='\\t')\n",
        "    print(f'Accuracy on Training set = {100*(correct/total):.6f}% [{correct}/{total}]') #Prints the Accuracy.\n",
        "\n",
        "    torch.save(model, '/content/gdrive/MyDrive/Kaggle/unzip/DR_ResNet50_test.pt')\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG6C4yH-qf48"
      },
      "outputs": [],
      "source": [
        "def validate(dataloader,model,loss_fn):\n",
        "    '''\n",
        "    validate function calculates the average loss per batch and the accuracy of the model's predictions.\n",
        "\n",
        "    Args :\n",
        "         dataloader: Iterator for the batches in the data_set.\n",
        "         model: Given an input produces an output by multiplying the input with the model weights.\n",
        "         loss_fn: Calculates the discrepancy between the label & the model's predictions.\n",
        "\n",
        "    Returns :\n",
        "         Average loss per batch which is calculated by dividing the losses for all the batches\n",
        "         with the number of batches.\n",
        "    '''\n",
        "\n",
        "    model.eval() #Sets the model for evaluation.\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad(): #No need to calculate the gradients.\n",
        "\n",
        "        for x,y in dataloader:\n",
        "\n",
        "            output        = model(x.to(device)) #model's output.\n",
        "            loss          = loss_fn(output,y.to(device)).item() #loss calculation.\n",
        "            running_loss += loss\n",
        "\n",
        "            total        += y.size(0)\n",
        "            predictions   = output.argmax(dim=1).cpu().detach()\n",
        "            correct      += (predictions == y.cpu().detach()).sum().item()\n",
        "            accuracy = 100*(correct/total)\n",
        "\n",
        "    avg_loss = running_loss/len(dataloader) #Average loss per batch.\n",
        "\n",
        "    print(f'\\nValidation Loss per batch = {avg_loss:.6f}',end='\\t')\n",
        "    print(f'Accuracy on Validation set = {100*(correct/total):.6f}% [{correct}/{total}]') #Prints the Accuracy.\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(train_losses, valid_losses, train_accuracies, valid_accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'g', label='Training Loss')\n",
        "    plt.plot(epochs, valid_losses, 'b', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, 'g', label='Training Accuracy')\n",
        "    plt.plot(epochs, valid_accuracies, 'b', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TIp6Sz0bAds2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U661TQ5wqYGU"
      },
      "outputs": [],
      "source": [
        "def optimize(train_dataloader,valid_dataloader,model,loss_fn,optimizer,nb_epochs, patience):\n",
        "    '''\n",
        "    optimize function calls the train & validate functions for (nb_epochs) times.\n",
        "\n",
        "    Args :\n",
        "        train_dataloader: DataLoader for the train_set.\n",
        "        valid_dataloader: DataLoader for the valid_set.\n",
        "        model: Given an input produces an output by multiplying the input with the model weights.\n",
        "        loss_fn: Calculates the discrepancy between the label & the model's predictions.\n",
        "        optimizer: Updates the model weights.\n",
        "        nb_epochs: Number of epochs.\n",
        "\n",
        "    Returns :\n",
        "        Tuple of lists containing losses for all the epochs.\n",
        "    '''\n",
        "    # Initialize the learning rate scheduler\n",
        "    # scheduler = StepLR(optimizer, step_size=4, gamma=0.5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "\n",
        "    #Lists to store losses for all the epochs.\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    train_accuracies = []\n",
        "    valid_accuracies = []\n",
        "\n",
        "    df = pd.DataFrame(columns=['epoch', 'train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy'])\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    no_improve_epoch = 0\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{nb_epochs}')\n",
        "        print('-------------------------------')\n",
        "        print(\"Epoch: %d, Learning Rate: %f \" % (epoch, optimizer.param_groups[0]['lr']))\n",
        "        train_loss, train_accuracy = train(train_dataloader,model,loss_fn,optimizer)\n",
        "        valid_loss, valid_accuracy = validate(valid_dataloader,model,loss_fn)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "        print(f'Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}')\n",
        "\n",
        "        plot_learning_curve(train_losses, valid_losses, train_accuracies, valid_accuracies)\n",
        "\n",
        "\n",
        "        # Check if the validation loss improved\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            no_improve_epoch = 0\n",
        "\n",
        "            # Save the model when validation loss improves\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            no_improve_epoch += 1\n",
        "\n",
        "        # If the validation loss did not improve for 'patience' epochs, stop training\n",
        "        if no_improve_epoch >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}, the validation loss did not improve for the last {patience} epochs')\n",
        "\n",
        "            # Save to CSV\n",
        "            df.to_csv('training_validation_metrics.csv', index=False)\n",
        "            break\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        # scheduler.step()\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        df = df.append({'epoch': epoch+1, 'train_loss': train_loss, 'train_accuracy': train_accuracy, 'valid_loss': valid_loss, 'valid_accuracy': valid_accuracy}, ignore_index=True)\n",
        "\n",
        "    print('\\nTraining has completed!')\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('training_validation_metrics.csv', index=False)\n",
        "\n",
        "    return train_losses,valid_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guN3gCYfqFdp"
      },
      "outputs": [],
      "source": [
        "loss_fn   = nn.CrossEntropyLoss(weight=class_weights) #CrossEntropyLoss with class_weights.\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.005, weight_decay=1e-5)\n",
        "nb_epochs = 500\n",
        "patience = 500\n",
        "\n",
        "#Call the optimize function.\n",
        "train_losses, valid_losses = optimize(train_dataloader,valid_dataloader,model,loss_fn,optimizer,nb_epochs, patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "culfm9CsH-w5"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/Kaggle/unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1AcQn3ikQyD"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"DR_ResNet50_test.pt\")\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Store all the model predictions for the test set\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# No need to track gradients for evaluation, saves memory and computations\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26QT6jxWj_07"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I92W90hYTAe"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# For multiclass case:\n",
        "num_classes = cm.shape[0]  # assuming cm is a square matrix\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = cm[i, i]\n",
        "    fn = cm[i, :].sum() - tp  # sum across the row, excluding the diagonal element\n",
        "    fp = cm[:, i].sum() - tp  # sum down the column, excluding the diagonal element\n",
        "    tn = cm.sum() - fn - fp - tp\n",
        "\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    print(f\"For class {i}, Sensitivity: {sensitivity}, Specificity: {specificity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grmA2x5vW406"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0OulKlKU_m4"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data.zip test train"
      ],
      "metadata": {
        "id": "eh5Tt7zRE4cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooZihzRhE9ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMXQy51n2KXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwtfH3HhE6FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edKrBcr75TIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwavX-r1IHAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mf8zd-ah2O9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__b1JrWyHoq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EB5YrKBbyRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTotw6mzRe_e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEUo1d9LRah0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}